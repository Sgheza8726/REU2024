/home/efleisig/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards: 100%|██████████████████████████████| 4/4 [00:02<00:00,  1.72it/s]
Map:   0%|                                                  | 0/2701 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "/home/efleisig/sams_reu/llama3_train.py", line 26, in <module>
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 602, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 567, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3156, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3547, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3416, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/efleisig/sams_reu/llama3_train.py", line 24, in tokenize_function
    return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=max_length)
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2803, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2889, in _call_one
    return self.batch_encode_plus(
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3071, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2708, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Traceback (most recent call last):
  File "/home/efleisig/sams_reu/llama3_train.py", line 26, in <module>
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 602, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 567, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3156, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3547, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/efleisig/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3416, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/efleisig/sams_reu/llama3_train.py", line 24, in tokenize_function
    return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=max_length)
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2803, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2889, in _call_one
    return self.batch_encode_plus(
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3071, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/efleisig/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2708, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.